{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oa_q_e_FkceG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRiMQIhiSnxo"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git datasets\n",
        "!pip install -q evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torch\n",
        "!pip show torchvision\n",
        "!pip show datasets\n",
        "!pip show albumentations"
      ],
      "metadata": {
        "id": "7YTfvYFh5amF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show transformers"
      ],
      "metadata": {
        "id": "rRGL-cxT58aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9VpgB6JYPDs"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESKRmsSnGTKy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgmUINtB3mWg"
      },
      "outputs": [],
      "source": [
        "!unzip drive/MyDrive/KCDH/data_S1.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7YFc9-3JnLz"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Function to load images and labels from directory\n",
        "def load_data(directory):\n",
        "    images = []\n",
        "    labels = []\n",
        "    image_dir = os.path.join(directory, \"images\")\n",
        "    label_dir = os.path.join(directory, \"labels\")\n",
        "    image_files = sorted(os.listdir(image_dir))\n",
        "    label_files = sorted(os.listdir(label_dir))\n",
        "    for image_file, label_file in zip(image_files, label_files):\n",
        "        # Assuming image and label files have corresponding names\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        label_path = os.path.join(label_dir, label_file)\n",
        "        image = Image.open(image_path)\n",
        "        label = Image.open(label_path)\n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "    return {\"image\": images, \"label\": labels}\n",
        "\n",
        "# Load train and validation data\n",
        "train_data = load_data(\"data_S1/train\")\n",
        "validation_data = load_data(\"data_S1/test\")\n",
        "\n",
        "# Create DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    \"train\": Dataset.from_dict(train_data),\n",
        "    \"validation\": Dataset.from_dict(validation_data)\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlLn66XD4iHA"
      },
      "source": [
        "Let's take a look at the dataset in more detail. It has a train and validation split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUILHbQmLMXe"
      },
      "outputs": [],
      "source": [
        "dataset_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yu6tEMdKeWq"
      },
      "outputs": [],
      "source": [
        "dataset_dict['train'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2te4Bfh94lqa"
      },
      "source": [
        "Let's take a look at the first training example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PROhIKLYt9D"
      },
      "outputs": [],
      "source": [
        "example = dataset_dict[\"train\"][1]\n",
        "image = example[\"image\"]\n",
        "# image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WHI4uZTYxQ3"
      },
      "outputs": [],
      "source": [
        "segmentation_map = example[\"label\"]\n",
        "# segmentation_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alIpQnrKM_Hg"
      },
      "outputs": [],
      "source": [
        "enhanced_segmentation_map = segmentation_map.point(lambda p: p*25)\n",
        "# enhanced_segmentation_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoXACbDL4ofn"
      },
      "source": [
        "In case of semantic segmentation, every pixel is labeled with a certain class. 0 is the \"background\" class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o1YlRPjY0YQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "segmentation_map = np.array(segmentation_map)\n",
        "segmentation_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLseyZbO4vl6"
      },
      "source": [
        "Let's load the mappings between integers and their classes (I got that from the [dataset card](https://huggingface.co/datasets/EduardoPacheco/FoodSeg103#data-categories) and asked an LLM to turn it into a dictionary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5V-4XT1Zd5x"
      },
      "outputs": [],
      "source": [
        "id2label = {\n",
        "    0: \"id_bg\",\n",
        "    1: \"id_1\",\n",
        "    2: \"id_2\",\n",
        "    3: \"id_3\",\n",
        "    4: \"id_4\",\n",
        "    5: \"id_5\",\n",
        "    6: \"id_6\",\n",
        "    7: \"id_7\",\n",
        "    8: \"id_8\",\n",
        "    9: \"id_9\",\n",
        "    10: \"id_10\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7hNoJWMZf8F"
      },
      "outputs": [],
      "source": [
        "print(id2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdN0jTvf5Al4"
      },
      "source": [
        "We can visualize the segmentation map on top of the image, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QCWRTScOF6M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# # map every class to a random color\n",
        "# id2color = {k: list(np.random.choice(range(256), size=3)) for k,v in id2label.items()}\n",
        "# id2color[0] = [0,0,0] # setting bg as black color\n",
        "# id2color\n",
        "\n",
        "# map every class to a random color\n",
        "cmap = plt.cm.get_cmap('viridis')  # Choose any colormap you prefer\n",
        "num_classes = 11\n",
        "colors = [cmap(i / num_classes) for i in range(num_classes)]\n",
        "id2color = {i: [int(c * 255) for c in color[:3]] for i, color in enumerate(colors)}\n",
        "id2color[0] = [0, 0, 0]  # Setting background as black color\n",
        "id2color"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display the colors for each class with their class IDs\n",
        "plt.figure(figsize=(10, 2))\n",
        "for i, (class_id, color) in enumerate(id2color.items()):\n",
        "    plt.subplot(1, num_classes, i + 1)\n",
        "    plt.imshow([[color]])\n",
        "    plt.title(f\"class {class_id}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TulJssAPFDZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaZKQUCj5DTU"
      },
      "outputs": [],
      "source": [
        "def visualize_map(image, segmentation_map):\n",
        "    color_seg = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
        "    for label, color in id2color.items():\n",
        "        color_seg[segmentation_map == label, :] = color\n",
        "\n",
        "    # Show image + mask\n",
        "    img = np.array(image) * 1 + color_seg * 0.5\n",
        "    img = img.astype(np.uint8)\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "visualize_map(image, segmentation_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_2Mt-7fZsgB"
      },
      "source": [
        "## Create PyTorch dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-vf9sfeZsZJ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "  def __init__(self, dataset, transform):\n",
        "    self.dataset = dataset\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.dataset[idx]\n",
        "    original_image = np.array(item[\"image\"])\n",
        "    original_segmentation_map = np.array(item[\"label\"])\n",
        "\n",
        "    transformed = self.transform(image=original_image, mask=original_segmentation_map)\n",
        "    image, target = torch.tensor(transformed['image']), torch.LongTensor(transformed['mask'])\n",
        "\n",
        "    # convert to C, H, W\n",
        "    image = image.permute(2,0,1)\n",
        "\n",
        "    return image, target, original_image, original_segmentation_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kzKLtFBmqg7"
      },
      "source": [
        "Let's create the training and validation datasets (note that we only randomly crop for training images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh_2YCRFaU3I"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "\n",
        "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
        "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    # hadded an issue with an image being too small to crop, PadIfNeeded didn't help...\n",
        "    # if anyone knows why this is happening I'm happy to read why\n",
        "    # A.PadIfNeeded(min_height=448, min_width=448),\n",
        "    # A.RandomResizedCrop(height=448, width=448),\n",
        "    A.Resize(width=448, height=448),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
        "], is_check_shapes=False)\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(width=448, height=448),\n",
        "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
        "\n",
        "], is_check_shapes=False)\n",
        "\n",
        "train_dataset = SegmentationDataset(dataset_dict[\"train\"], transform=train_transform)\n",
        "val_dataset = SegmentationDataset(dataset_dict[\"validation\"], transform=val_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqDEyazEbF5e"
      },
      "outputs": [],
      "source": [
        "pixel_values, target, original_image, original_segmentation_map = train_dataset[3]\n",
        "print(pixel_values.shape)\n",
        "print(target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVLCg341bTIO"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(original_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L0yRy-WWTrH"
      },
      "outputs": [],
      "source": [
        "[id2label[id] for id in np.unique(original_segmentation_map).tolist()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1iVTCfPbcdd"
      },
      "source": [
        "## Create PyTorch dataloaders\n",
        "\n",
        "Next, we create PyTorch dataloaders, which allow us to get batches of data (as neural networks are trained on batches using stochastic gradient descent or SGD). We just stack the various images and labels along a new batch dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an-6CubjbcR5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(inputs):\n",
        "    batch = dict()\n",
        "    batch[\"pixel_values\"] = torch.stack([i[0] for i in inputs], dim=0)\n",
        "    batch[\"labels\"] = torch.stack([i[1] for i in inputs], dim=0)\n",
        "    batch[\"original_images\"] = [i[2] for i in inputs]\n",
        "    batch[\"original_segmentation_maps\"] = [i[3] for i in inputs]\n",
        "    return batch\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=3, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=3, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeoRw_UdnBkv"
      },
      "source": [
        "Let's check a batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sojhyWCEbmrO"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "for k,v in batch.items():\n",
        "  if isinstance(v,torch.Tensor):\n",
        "    print(k,v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(val_dataloader))\n",
        "for k,v in batch.items():\n",
        "  if isinstance(v,torch.Tensor):\n",
        "    print(k,v.shape)"
      ],
      "metadata": {
        "id": "FL8zeoyfVdZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNNoEBH_nEv1"
      },
      "source": [
        "Note that the pixel values are float32 tensors, whereas the labels are long tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFxBeSe1hE_e"
      },
      "outputs": [],
      "source": [
        "batch[\"pixel_values\"].dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5gIzIfQhGmX"
      },
      "outputs": [],
      "source": [
        "batch[\"labels\"].dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohbpttyK1J6P"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "unnormalized_image = (batch[\"pixel_values\"][0].numpy() * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\n",
        "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
        "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
        "unnormalized_image = Image.fromarray(unnormalized_image)\n",
        "unnormalized_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P_RLnG21HPt"
      },
      "outputs": [],
      "source": [
        "[id2label[id] for id in torch.unique(batch[\"labels\"][0]).tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNwoemVO1qKZ"
      },
      "outputs": [],
      "source": [
        "visualize_map(unnormalized_image, batch[\"labels\"][0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYjFXVzZSpmP"
      },
      "source": [
        "## Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLzR_mt_SnE2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Dinov2Model, Dinov2PreTrainedModel\n",
        "from transformers.modeling_outputs import SemanticSegmenterOutput\n",
        "\n",
        "class LinearClassifier(torch.nn.Module):\n",
        "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=1):\n",
        "        super(LinearClassifier, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.width = tokenW\n",
        "        self.height = tokenH\n",
        "        self.classifier = torch.nn.Conv2d(in_channels, num_labels, (1,1))\n",
        "\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
        "        embeddings = embeddings.permute(0,3,1,2)\n",
        "\n",
        "        return self.classifier(embeddings)\n",
        "\n",
        "\n",
        "class Dinov2ForSemanticSegmentation(Dinov2PreTrainedModel):\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "\n",
        "    self.dinov2 = Dinov2Model(config)\n",
        "    self.classifier = LinearClassifier(config.hidden_size, 32, 32, config.num_labels)\n",
        "\n",
        "  def forward(self, pixel_values, output_hidden_states=False, output_attentions=False, labels=None):\n",
        "    # use frozen features\n",
        "    outputs = self.dinov2(pixel_values,\n",
        "                            output_hidden_states=output_hidden_states,\n",
        "                            output_attentions=output_attentions)\n",
        "    # get the patch embeddings - so we exclude the CLS token\n",
        "    patch_embeddings = outputs.last_hidden_state[:,1:,:]\n",
        "\n",
        "    # convert to logits and upsample to the size of the pixel values\n",
        "    logits = self.classifier(patch_embeddings)\n",
        "    logits = torch.nn.functional.interpolate(logits, size=pixel_values.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      # important: we're going to use 0 here as ignore index instead of the default -100\n",
        "      # as we don't want the model to learn to predict background\n",
        "      loss_fct = torch.nn.CrossEntropyLoss(ignore_index=0) # change it to -100\n",
        "    #   loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "      loss = loss_fct(logits, labels)\n",
        "\n",
        "    return SemanticSegmenterOutput(\n",
        "        loss=loss,\n",
        "        logits=logits,\n",
        "        hidden_states=outputs.hidden_states,\n",
        "        attentions=outputs.attentions,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fqZKKdHasMlD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zQ7UUj0NAX7"
      },
      "source": [
        "We can instantiate the model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGsb2T-DUuVn"
      },
      "outputs": [],
      "source": [
        "model = Dinov2ForSemanticSegmentation.from_pretrained(\"facebook/dinov2-base\", id2label=id2label, num_labels=len(id2label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WGy3seL3VFT"
      },
      "source": [
        "Important: we don't want to train the DINOv2 backbone, only the linear classification head. Hence we don't want to track any gradients for the backbone parameters. This will greatly save us in terms of memory used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VANFfhb23KEJ"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "  if name.startswith(\"dinov2\"):\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJZYos76HP0B"
      },
      "source": [
        "Let's perform a forward pass on a random batch, to verify the shape of the logits, verify we can calculate a loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEwaZoCDWfc9"
      },
      "outputs": [],
      "source": [
        "outputs = model(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n",
        "print(outputs.logits.shape)\n",
        "print(outputs.loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRb97qtd3x03"
      },
      "source": [
        "As can be seen, the logits are of shape (batch_size, num_labels, height, width). We can then just take the highest logit (score) for each pixel as the model's prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjr2SZbCSvQv"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "We'll train the model in regular PyTorch fashion. We also use the mIoU (mean Intersection-over-Union) metric to evaluate the performance during training.\n",
        "\n",
        "Note that I made this entire notebook just for demo purposes, I haven't done any hyperparameter tuning, so feel free to improve. You can also of course use other training frameworks (like the ðŸ¤— Trainer, PyTorch Lightning, ðŸ¤— Accelerate, ...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYwgRoc4FEXO"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"mean_iou\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfgQUnMbCIrs"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# training hyperparameters\n",
        "# NOTE: I've just put some random ones here, not optimized at all\n",
        "# feel free to experiment, see also DINOv2 paper\n",
        "learning_rate = 5e-3\n",
        "#epochs = 20\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# put model on GPU (set runtime to GPU in Google Colab)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
        "\n",
        "# Define the search space\n",
        "space = {\n",
        "    'learning_rate': hp.choice('learning_rate', [1e-3, 5e-4, 1e-4]),\n",
        "    'max_epochs': hp.choice('max_epochs', [10, 20, 30])\n",
        "}\n",
        "\n",
        "# Define the objective function\n",
        "def objective(params):\n",
        "    learning_rate = params['learning_rate']\n",
        "    max_epochs = params['max_epochs']\n",
        "\n",
        "    # Train the model and calculate validation loss (not shown for brevity)\n",
        "    # Use a validation dataset or cross-validation\n",
        "\n",
        "    val_loss = compute_validation_loss(model, val_dataloader, device)\n",
        "\n",
        "    return {'loss': val_loss, 'status': STATUS_OK}\n",
        "\n",
        "# Initialize Trials object\n",
        "trials = Trials()\n",
        "\n",
        "# Perform Bayesian Optimization\n",
        "best = fmin(fn=objective,\n",
        "            space=space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=10,  # Number of optimization iterations\n",
        "            trials=trials)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "best_learning_rate = space['learning_rate'][best['learning_rate']]\n",
        "best_max_epochs = space['max_epochs'][best['max_epochs']]\n",
        "print(f\"Best Learning Rate: {best_learning_rate}, Best Max Epochs: {best_max_epochs}\")\n"
      ],
      "metadata": {
        "id": "zsdKaUbqwSIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2fEreo0Svy1"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the number of epochs\n",
        "max_epochs = 100  # Set a high upper limit for epochs\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5 # Number of epochs to wait for improvement\n",
        "min_delta = 0.001  # Minimum change to qualify as an improvement\n",
        "best_val_loss = np.inf\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Function to compute validation loss\n",
        "def compute_validation_loss(model, val_dataloader, device):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(pixel_values, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "    return val_loss / len(val_dataloader)\n",
        "\n",
        "# Put model in training mode\n",
        "model.train()\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Print shapes\n",
        "        print(\"Logits shape:\", outputs.logits.shape)\n",
        "        print(\"Labels shape:\", labels.shape)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Evaluate\n",
        "        with torch.no_grad():\n",
        "            predicted = outputs.logits.argmax(dim=1)\n",
        "\n",
        "            # Note that the metric expects predictions + labels as numpy arrays\n",
        "            metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
        "\n",
        "        # Let's print loss and metrics every batch\n",
        "        if idx % 1 == 0:\n",
        "            metrics = metric.compute(num_labels=len(id2label),\n",
        "                                     ignore_index=0,\n",
        "                                     reduce_labels=False)\n",
        "            print(\"idx:\", idx)\n",
        "            print(\"Loss:\", loss.item())\n",
        "            print(\"Mean_iou:\", metrics[\"mean_iou\"])\n",
        "            print(\"Mean accuracy:\", metrics[\"mean_accuracy\"])\n",
        "            print(\"------------------------------------------\")\n",
        "\n",
        "    # Validation loss computation\n",
        "    val_loss = compute_validation_loss(model, val_dataloader, device)\n",
        "    print(f\"Validation Loss: {val_loss}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss - min_delta:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTtl598Ri4I2"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Once we've trained a model, we can perform inference on new images as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYnoX7-0i_V1"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "test_image = dataset_dict[\"validation\"][6][\"image\"]\n",
        "test_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DycD-PIdWs3q"
      },
      "outputs": [],
      "source": [
        "pixel_values = val_transform(image=np.array(test_image))[\"image\"]\n",
        "pixel_values = torch.tensor(pixel_values)\n",
        "pixel_values = pixel_values.permute(2,0,1).unsqueeze(0) # convert to (batch_size, num_channels, height, width)\n",
        "print(pixel_values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUA_OJtCjUtz"
      },
      "outputs": [],
      "source": [
        "# forward pass\n",
        "with torch.no_grad():\n",
        "  outputs = model(pixel_values.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-s9RbpsjXT6"
      },
      "outputs": [],
      "source": [
        "upsampled_logits = torch.nn.functional.interpolate(outputs.logits,\n",
        "                                                   size=test_image.size[::-1],\n",
        "                                                   mode=\"bilinear\", align_corners=False)\n",
        "predicted_map = upsampled_logits.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_map(dataset_dict[\"validation\"][6][\"image\"], np.array(dataset_dict[\"validation\"][6][\"label\"]))"
      ],
      "metadata": {
        "id": "bxrSrCE4XK5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_map(test_image, predicted_map.squeeze().cpu())"
      ],
      "metadata": {
        "id": "YDwqSNhyW8jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKRDduqtkZp1"
      },
      "outputs": [],
      "source": [
        "visualize_map(test_image, predicted_map.squeeze().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-2VZY_gVYiX"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6CA8J7zuSdo"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj50ne_9uUc8"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2QaFSwgHU84"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSxsEOJFHkH7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}