{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa_q_e_FkceG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_MZ2M4XM2e4"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git datasets\n",
        "!pip install -q evaluate\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YTfvYFh5amF"
      },
      "outputs": [],
      "source": [
        "!pip show torch\n",
        "!pip show torchvision\n",
        "!pip show datasets\n",
        "!pip show albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRGL-cxT58aK"
      },
      "outputs": [],
      "source": [
        "!pip show transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9VpgB6JYPDs"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESKRmsSnGTKy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgmUINtB3mWg"
      },
      "outputs": [],
      "source": [
        "!unzip -o drive/MyDrive/KCDH/data_S1.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7YFc9-3JnLz"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Function to load images and labels from directory\n",
        "def load_data(directory):\n",
        "    images = []\n",
        "    labels = []\n",
        "    image_dir = os.path.join(directory, \"images\")\n",
        "    label_dir = os.path.join(directory, \"labels\")\n",
        "    image_files = sorted(os.listdir(image_dir))\n",
        "    label_files = sorted(os.listdir(label_dir))\n",
        "    for image_file, label_file in zip(image_files, label_files):\n",
        "        # Assuming image and label files have corresponding names\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        label_path = os.path.join(label_dir, label_file)\n",
        "        image = Image.open(image_path)\n",
        "        label = Image.open(label_path)\n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "    return {\"image\": images, \"label\": labels}\n",
        "\n",
        "# Load train and validation data\n",
        "train_data = load_data(\"data_S1/train\")\n",
        "validation_data = load_data(\"data_S1/test\")\n",
        "\n",
        "# Create DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    \"train\": Dataset.from_dict(train_data),\n",
        "    \"validation\": Dataset.from_dict(validation_data)\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlLn66XD4iHA"
      },
      "source": [
        "Let's take a look at the dataset in more detail. It has a train and validation split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUILHbQmLMXe"
      },
      "outputs": [],
      "source": [
        "dataset_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yu6tEMdKeWq"
      },
      "outputs": [],
      "source": [
        "dataset_dict['train'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2te4Bfh94lqa"
      },
      "source": [
        "Let's take a look at the first training example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PROhIKLYt9D"
      },
      "outputs": [],
      "source": [
        "example = dataset_dict[\"train\"][1]\n",
        "image = example[\"image\"]\n",
        "# image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WHI4uZTYxQ3"
      },
      "outputs": [],
      "source": [
        "segmentation_map = example[\"label\"]\n",
        "# segmentation_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alIpQnrKM_Hg"
      },
      "outputs": [],
      "source": [
        "enhanced_segmentation_map = segmentation_map.point(lambda p: p*25)\n",
        "# enhanced_segmentation_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoXACbDL4ofn"
      },
      "source": [
        "In case of semantic segmentation, every pixel is labeled with a certain class. 0 is the \"background\" class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6o1YlRPjY0YQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "segmentation_map = np.array(segmentation_map)\n",
        "segmentation_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLseyZbO4vl6"
      },
      "source": [
        "Let's load the mappings between integers and their classes (I got that from the [dataset card](https://huggingface.co/datasets/EduardoPacheco/FoodSeg103#data-categories) and asked an LLM to turn it into a dictionary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5V-4XT1Zd5x"
      },
      "outputs": [],
      "source": [
        "id2label = {\n",
        "    0: \"id_bg\",\n",
        "    1: \"id_1\",\n",
        "    2: \"id_2\",\n",
        "    3: \"id_3\",\n",
        "    4: \"id_4\",\n",
        "    5: \"id_5\",\n",
        "    6: \"id_6\",\n",
        "    7: \"id_7\",\n",
        "    8: \"id_8\",\n",
        "    9: \"id_9\",\n",
        "    10: \"id_10\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7hNoJWMZf8F"
      },
      "outputs": [],
      "source": [
        "print(id2label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdN0jTvf5Al4"
      },
      "source": [
        "We can visualize the segmentation map on top of the image, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QCWRTScOF6M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# # map every class to a random color\n",
        "# id2color = {k: list(np.random.choice(range(256), size=3)) for k,v in id2label.items()}\n",
        "# id2color[0] = [0,0,0] # setting bg as black color\n",
        "# id2color\n",
        "\n",
        "# map every class to a random color\n",
        "cmap = plt.cm.get_cmap('viridis')  # Choose any colormap you prefer\n",
        "num_classes = 11\n",
        "colors = [cmap(i / num_classes) for i in range(num_classes)]\n",
        "id2color = {i: [int(c * 255) for c in color[:3]] for i, color in enumerate(colors)}\n",
        "id2color[0] = [0, 0, 0]  # Setting background as black color\n",
        "id2color"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TulJssAPFDZ6"
      },
      "outputs": [],
      "source": [
        "# display the colors for each class with their class IDs\n",
        "plt.figure(figsize=(10, 2))\n",
        "for i, (class_id, color) in enumerate(id2color.items()):\n",
        "    plt.subplot(1, num_classes, i + 1)\n",
        "    plt.imshow([[color]])\n",
        "    plt.title(f\"class {class_id}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaZKQUCj5DTU"
      },
      "outputs": [],
      "source": [
        "def visualize_map(image, segmentation_map):\n",
        "    color_seg = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
        "    for label, color in id2color.items():\n",
        "        color_seg[segmentation_map == label, :] = color\n",
        "\n",
        "    # Show image + mask\n",
        "    img = np.array(image) * 1 + color_seg * 0.5\n",
        "    img = img.astype(np.uint8)\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "visualize_map(image, segmentation_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_2Mt-7fZsgB"
      },
      "source": [
        "## Create PyTorch dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-vf9sfeZsZJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "  def __init__(self, dataset, transform):\n",
        "    self.dataset = dataset\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.dataset[idx]\n",
        "    original_image = np.array(item[\"image\"])\n",
        "    original_segmentation_map = np.array(item[\"label\"])\n",
        "\n",
        "    transformed = self.transform(image=original_image, mask=original_segmentation_map)\n",
        "    image, target = torch.tensor(transformed['image']), torch.LongTensor(transformed['mask'])\n",
        "\n",
        "    # convert to C, H, W\n",
        "    image = image.permute(2,0,1)\n",
        "\n",
        "    return image, target, original_image, original_segmentation_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kzKLtFBmqg7"
      },
      "source": [
        "Let's create the training and validation datasets (note that we only randomly crop for training images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh_2YCRFaU3I"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "\n",
        "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
        "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    # hadded an issue with an image being too small to crop, PadIfNeeded didn't help...\n",
        "    # if anyone knows why this is happening I'm happy to read why\n",
        "    # A.PadIfNeeded(min_height=448, min_width=448),\n",
        "    # A.RandomResizedCrop(height=448, width=448),\n",
        "    A.Resize(width=448, height=448),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
        "], is_check_shapes=False)\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(width=448, height=448),\n",
        "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
        "\n",
        "], is_check_shapes=False)\n",
        "\n",
        "train_dataset = SegmentationDataset(dataset_dict[\"train\"], transform=train_transform)\n",
        "val_dataset = SegmentationDataset(dataset_dict[\"validation\"], transform=val_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqDEyazEbF5e"
      },
      "outputs": [],
      "source": [
        "pixel_values, target, original_image, original_segmentation_map = train_dataset[3]\n",
        "print(pixel_values.shape)\n",
        "print(target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVLCg341bTIO"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(original_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L0yRy-WWTrH"
      },
      "outputs": [],
      "source": [
        "[id2label[id] for id in np.unique(original_segmentation_map).tolist()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1iVTCfPbcdd"
      },
      "source": [
        "## Create PyTorch dataloaders\n",
        "\n",
        "Next, we create PyTorch dataloaders, which allow us to get batches of data (as neural networks are trained on batches using stochastic gradient descent or SGD). We just stack the various images and labels along a new batch dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an-6CubjbcR5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Initialize the Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "# Define your collate function\n",
        "def collate_fn(inputs):\n",
        "    batch = dict()\n",
        "    batch[\"pixel_values\"] = torch.stack([i[0] for i in inputs], dim=0)\n",
        "    batch[\"labels\"] = torch.stack([i[1] for i in inputs], dim=0)\n",
        "    batch[\"original_images\"] = [i[2] for i in inputs]\n",
        "    batch[\"original_segmentation_maps\"] = [i[3] for i in inputs]\n",
        "    return batch\n",
        "\n",
        "# Wrap your dataloaders with Accelerator\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=3, shuffle=True, collate_fn=collate_fn)\n",
        "train_dataloader = accelerator.prepare(train_dataloader)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=3, shuffle=False, collate_fn=collate_fn)\n",
        "val_dataloader = accelerator.prepare(val_dataloader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeoRw_UdnBkv"
      },
      "source": [
        "Let's check a batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sojhyWCEbmrO"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "for k,v in batch.items():\n",
        "  if isinstance(v,torch.Tensor):\n",
        "    print(k,v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL8zeoyfVdZd"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(val_dataloader))\n",
        "for k,v in batch.items():\n",
        "  if isinstance(v,torch.Tensor):\n",
        "    print(k,v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNNoEBH_nEv1"
      },
      "source": [
        "Note that the pixel values are float32 tensors, whereas the labels are long tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFxBeSe1hE_e"
      },
      "outputs": [],
      "source": [
        "batch[\"pixel_values\"].dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5gIzIfQhGmX"
      },
      "outputs": [],
      "source": [
        "batch[\"labels\"].dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohbpttyK1J6P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Ensure the tensor is on the CPU\n",
        "pixel_values_cpu = batch[\"pixel_values\"][0].cpu()\n",
        "\n",
        "# Unnormalize the image\n",
        "unnormalized_image = (pixel_values_cpu.numpy() * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\n",
        "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
        "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
        "unnormalized_image = Image.fromarray(unnormalized_image)\n",
        "unnormalized_image.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P_RLnG21HPt"
      },
      "outputs": [],
      "source": [
        "\n",
        " [id2label[id] for id in torch.unique(batch[\"labels\"][0]).tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNwoemVO1qKZ"
      },
      "outputs": [],
      "source": [
        "# Ensure the labels tensor is on the CPU\n",
        "labels_cpu = batch[\"labels\"][0].cpu().numpy()\n",
        "\n",
        "visualize_map(unnormalized_image, labels_cpu)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYjFXVzZSpmP"
      },
      "source": [
        "## Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLzR_mt_SnE2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import Dinov2Model, Dinov2PreTrainedModel\n",
        "from transformers.modeling_outputs import SemanticSegmenterOutput\n",
        "\n",
        "\n",
        "\n",
        "class DeconvDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DeconvDecoder, self).__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, out_channels, kernel_size=4, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        B, N, C = embeddings.shape\n",
        "        H = W = int(N**0.5)\n",
        "        if H * W != N:\n",
        "            raise ValueError(\"The number of patches is not a perfect square\")\n",
        "        embeddings = embeddings.permute(0, 2, 1).reshape(B, C, H, W)\n",
        "        return self.decoder(embeddings)\n",
        "\n",
        "class Dinov2ForSemanticSegmentation(Dinov2PreTrainedModel):\n",
        "    def __init__(self, config, num_labels=11, **kwargs):\n",
        "        super().__init__(config)\n",
        "        self.dinov2 = Dinov2Model(config)\n",
        "        self.decoder = DeconvDecoder(config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, pixel_values, output_hidden_states=False, output_attentions=False, labels=None):\n",
        "        outputs = self.dinov2(pixel_values, output_hidden_states=output_hidden_states, output_attentions=output_attentions)\n",
        "        patch_embeddings = outputs.last_hidden_state[:, 1:, :]  # Exclude CLS token\n",
        "        logits = self.decoder(patch_embeddings)\n",
        "\n",
        "        logits = torch.nn.functional.interpolate(logits, size=pixel_values.shape[2:], mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            assert torch.max(labels) < logits.shape[1], f\"Max label {torch.max(labels)} is greater than the number of classes {logits.shape[1]}\"\n",
        "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return SemanticSegmenterOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# config = Dinov2Config.from_pretrained('path/to/config')\n",
        "# model = HybridSemanticSegmentation(dinov2_config=config, openclip_model_name=\"ViT-B-32\", openclip_pretrained=\"openai\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqZKKdHasMlD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zQ7UUj0NAX7"
      },
      "source": [
        "We can instantiate the model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGsb2T-DUuVn"
      },
      "outputs": [],
      "source": [
        "model = Dinov2ForSemanticSegmentation.from_pretrained(\"facebook/dinov2-base\", id2label=id2label, num_labels=len(id2label))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WGy3seL3VFT"
      },
      "source": [
        "Important: we don't want to train the DINOv2 backbone, only the linear classification head. Hence we don't want to track any gradients for the backbone parameters. This will greatly save us in terms of memory used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VANFfhb23KEJ"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "  if name.startswith(\"dinov2\"):\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJZYos76HP0B"
      },
      "source": [
        "Let's perform a forward pass on a random batch, to verify the shape of the logits, verify we can calculate a loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEwaZoCDWfc9"
      },
      "outputs": [],
      "source": [
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the same device as the input tensors\n",
        "model.to(device)\n",
        "\n",
        "# Ensure that the input tensors are on the same device as the model\n",
        "pixel_values = batch[\"pixel_values\"].to(device)\n",
        "labels = batch[\"labels\"].to(device)\n",
        "\n",
        "# Pass the tensors to the model\n",
        "outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "print(outputs.logits.shape)\n",
        "print(outputs.loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPte17vvOnIg"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRb97qtd3x03"
      },
      "source": [
        "As can be seen, the logits are of shape (batch_size, num_labels, height, width). We can then just take the highest logit (score) for each pixel as the model's prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjr2SZbCSvQv"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "We'll train the model in regular PyTorch fashion. We also use the mIoU (mean Intersection-over-Union) metric to evaluate the performance during training.\n",
        "\n",
        "Note that I made this entire notebook just for demo purposes, I haven't done any hyperparameter tuning, so feel free to improve. You can also of course use other training frameworks (like the ðŸ¤— Trainer, PyTorch Lightning, ðŸ¤— Accelerate, ...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYwgRoc4FEXO"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "metric = evaluate.load(\"mean_iou\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD1H1lK92Cm9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Define the Decoder and Dinov2ForSemanticSegmentation classes here...\n",
        "\n",
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfgQUnMbCIrs"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "!pip install torch torchvision torchaudio\n",
        "\n",
        "\n",
        "# training hyperparameters\n",
        "# NOTE: I've just put some random ones here, not optimized at all\n",
        "# feel free to experiment, see also DINOv2 paper\n",
        "#learning_rate = 5e-3\n",
        "#epochs = 20\n",
        "#optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# put model on GPU (set runtime to GPU in Google Colab)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsdKaUbqwSIi"
      },
      "outputs": [],
      "source": [
        "pip install optuna\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZxchXWahP9j"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glyPHuAe5F2c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "import optuna\n",
        "import numpy as np\n",
        "warmup_steps = 800\n",
        "\n",
        "# Define a cosine learning rate scheduler\n",
        "class WarmupCosineSchedule(optim.lr_scheduler.LambdaLR):\n",
        "    def __init__(self, optimizer, warmup_steps, total_steps, min_lr, max_lr):\n",
        "        def lr_lambda(current_step):\n",
        "            if current_step < warmup_steps:\n",
        "                return float(current_step) / float(warmup_steps)\n",
        "            progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "            return min_lr + 0.5 * (max_lr - min_lr) * (1. + np.cos(np.pi * progress))\n",
        "        super().__init__(optimizer, lr_lambda)\n",
        "\n",
        "# Define the momentum scheduler\n",
        "class MomentumSchedule:\n",
        "    def __init__(self, start_momentum, end_momentum, total_steps):\n",
        "        self.start_momentum = start_momentum\n",
        "        self.end_momentum = end_momentum\n",
        "        self.total_steps = total_steps\n",
        "\n",
        "    def get_momentum(self, step):\n",
        "        progress = float(step) / float(self.total_steps)\n",
        "        return self.start_momentum + (self.end_momentum - self.start_momentum) * 0.5 * (1. + np.cos(np.pi * progress))\n",
        "\n",
        "# Run hyperparameter optimization with Optuna\n",
        "def objective(trial):\n",
        "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 0.00887, 0.1)\n",
        "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-5, 1e-3)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    total_steps = 4000  # Reduced from 625000\n",
        "    warmup_steps = 800  # Reduced from 100000\n",
        "    lr_scheduler = WarmupCosineSchedule(optimizer, warmup_steps, total_steps, min_lr=0.04, max_lr=0.2)\n",
        "    momentum_scheduler = MomentumSchedule(0.994, 1.0, total_steps)\n",
        "\n",
        "    epochs = 8  # Further reduced the number of epochs for faster tuning\n",
        "    early_stopping_patience = 2  # Reduced the patience for early stopping for faster tuning\n",
        "    best_mean_iou = -1\n",
        "    best_model_state_dict = None\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        predictions = []\n",
        "        true_labels = []\n",
        "        with torch.no_grad():\n",
        "            for val_batch in val_dataloader:\n",
        "                val_pixel_values = val_batch[\"pixel_values\"].to(device)\n",
        "                val_labels = val_batch[\"labels\"].to(device)\n",
        "                val_outputs = model(val_pixel_values, labels=val_labels)\n",
        "                predictions.extend(torch.argmax(val_outputs.logits, dim=1).cpu().numpy())\n",
        "                true_labels.extend(val_labels.cpu().numpy())\n",
        "\n",
        "        for idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(pixel_values, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        total_loss /= len(train_dataloader)\n",
        "\n",
        "        metrics = compute_metrics(predictions, true_labels, num_labels=len(id2color))\n",
        "        binary_iou = metrics[\"binary_iou\"]\n",
        "        binary_accuracy = metrics[\"binary_accuracy\"]\n",
        "        mean_iou = metrics[\"mean_iou\"]\n",
        "        mean_accuracy = metrics[\"mean_accuracy\"]\n",
        "        per_class_iou_list = metrics[\"per_class_iou_list\"]\n",
        "        per_class_accuracy_list = metrics[\"per_class_accuracy_list\"]\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, Loss: {total_loss:.4f}, Mean IoU (Multi-class): {mean_iou:.4f}, Accuracy (Multi-class): {mean_accuracy:.4f}, Mean IoU (Binary): {binary_iou:.4f}, Accuracy (Binary): {binary_accuracy:.4f}\")\n",
        "        for cls, (iou, accuracy) in enumerate(zip(per_class_iou_list[1:], per_class_accuracy_list), start=1):\n",
        "            print(f\"Class {cls }: IoU = {iou:.4f}, Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "        trial.report(mean_iou, epoch)\n",
        "\n",
        "        if mean_iou > best_mean_iou:\n",
        "            best_mean_iou = mean_iou\n",
        "            best_model_state_dict = model.state_dict()\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    return best_mean_iou\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=3)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "print(study.best_trial)\n",
        "# Adjust the number of trials as needed\n",
        "\n",
        "# Get best hyperparameters\n",
        "best_params = study.best_params\n",
        "best_learning_rate = best_params[\"learning_rate\"]\n",
        "best_weight_decay = best_params[\"weight_decay\"]\n",
        "\n",
        "# Train with best hyperparameters\n",
        "optimizer = AdamW(model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay)\n",
        "lr_scheduler = WarmupCosineSchedule(optimizer, warmup_steps, total_steps, min_lr=0.04, max_lr=0.2)\n",
        "momentum_scheduler = MomentumSchedule(0.994, 1.0, total_steps)\n",
        "\n",
        "# Load the best model state dict\n",
        "model.load_state_dict(best_model_state_dict)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTtl598Ri4I2"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Once we've trained a model, we can perform inference on new images as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYnoX7-0i_V1"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "test_image = dataset_dict[\"validation\"][6][\"image\"]\n",
        "test_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DycD-PIdWs3q"
      },
      "outputs": [],
      "source": [
        "pixel_values = val_transform(image=np.array(test_image))[\"image\"]\n",
        "pixel_values = torch.tensor(pixel_values)\n",
        "pixel_values = pixel_values.permute(2,0,1).unsqueeze(0) # convert to (batch_size, num_channels, height, width)\n",
        "print(pixel_values.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUA_OJtCjUtz"
      },
      "outputs": [],
      "source": [
        "# forward pass\n",
        "with torch.no_grad():\n",
        "  outputs = model(pixel_values.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-s9RbpsjXT6"
      },
      "outputs": [],
      "source": [
        "upsampled_logits = torch.nn.functional.interpolate(outputs.logits,\n",
        "                                                   size=test_image.size[::-1],\n",
        "                                                   mode=\"bilinear\", align_corners=False)\n",
        "predicted_map = upsampled_logits.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5EXdKYJBPcX"
      },
      "outputs": [],
      "source": [
        "def visualize_map(image, segmentation_map, id2color):\n",
        "    # Reshape segmentation_map to 2D if needed\n",
        "    if len(segmentation_map.shape) == 1:\n",
        "        side_length = int(np.sqrt(segmentation_map.shape[0]))\n",
        "        segmentation_map = segmentation_map.reshape((side_length, side_length))\n",
        "\n",
        "    # Ensure segmentation_map is a 2D array\n",
        "    if len(segmentation_map.shape) != 2:\n",
        "        raise ValueError(\"Segmentation map must be a 2D array.\")\n",
        "\n",
        "    color_seg = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
        "    for label, color in id2color.items():\n",
        "        color_seg[segmentation_map == label, :] = color\n",
        "\n",
        "    # Show image + mask\n",
        "    img = np.array(image) * 1 + color_seg * 0.5\n",
        "    img = img.astype(np.uint8)\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4CfbHDjKH14"
      },
      "outputs": [],
      "source": [
        "# Assuming `test_image` is the original image and `compute_metrics` is a function that computes metrics\n",
        "visualize_map(test_image, predicted_map.squeeze(), id2color)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-2VZY_gVYiX"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6CA8J7zuSdo"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj50ne_9uUc8"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2QaFSwgHU84"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSxsEOJFHkH7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}